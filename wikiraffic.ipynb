{
  "metadata": {
    "language_info": {
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "nbconvert_exporter": "python",
      "mimetype": "text/x-python",
      "name": "python",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "version": "3.6.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "cells": [
    {
      "metadata": {
        "_uuid": "125d0f27f176e872e94b5c26778cb34219440af6",
        "_cell_guid": "acd78cc4-5a85-41ae-bffe-50bef0847fb2"
      },
      "cell_type": "markdown",
      "source": [
        "## This Notebook is to visualize the data and train an ARIMA model for each language category combined. The outputs from this model can be used as an input feature for an ensemble of models based on their page language.\n",
        "\n",
        "### Note: This is an extension of currently existing kernel https://www.kaggle.com/muonneutrino/wikipedia-traffic-data-exploration"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "68dd6b00003be25e4faba8cd4ce379430655662b",
        "_cell_guid": "4840662b-2730-4149-9f45-8c6ca424de82"
      },
      "cell_type": "markdown",
      "source": [
        "### Importing all the libraries we will be using for visualization and training"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "1b100b7e5361d81b846d8ef32eb7fca73e4c699e",
        "_cell_guid": "1328926e-4cda-45cb-95d3-ebe1658df8c2"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import re # to separate pages based on language (regular expression)\n",
        "import matplotlib.pyplot as plt # to visualize data\n",
        "from pandas.tools.plotting import autocorrelation_plot # to visualize and configure the parameters of ARIMA model\n",
        "from statsmodels.tsa.arima_model import ARIMA # to make an ARIMA model that fits the data"
      ]
    },
    {
      "metadata": {
        "_uuid": "3799cfc63206175d06bd2e2a4d3db52e37208b62",
        "_cell_guid": "5da4a6ef-fbb2-455c-b60d-9dd8c56c69f8"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the training data as a pandas DataFrame"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "23c16cdccec1a822a5572e7941c52a37477849fc",
        "_cell_guid": "b874b339-ccb9-4204-b00e-fd457d8e7f87"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "train_df = pd.read_csv('../input/train_1.csv').fillna(0)\n",
        "train_df.head()"
      ]
    },
    {
      "metadata": {
        "_uuid": "3151f66b33ce9ffd356bccbf7c48f77c64d65c95",
        "_cell_guid": "8c76bb20-1525-4f67-8838-fbe84cd63988"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "train_df.info()"
      ]
    },
    {
      "metadata": {
        "_uuid": "c3a9f0a88b6f9202cfa6e84079beb08091d43907",
        "_cell_guid": "f80d5f97-0e7c-487a-b4dc-535404a572f2"
      },
      "cell_type": "markdown",
      "source": [
        "### Simple code to get the language of any given page"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "932101ffc8467879fbb00be730bc4dc13bd110b3",
        "collapsed": true,
        "_cell_guid": "73123a60-bf2a-4aa2-a826-a5384df34627"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "def find_language(url):\n",
        "    res = re.search('[a-z][a-z].wikipedia.org',url)\n",
        "    if res:\n",
        "        return res[0][0:2]\n",
        "    return 'na'\n",
        "\n",
        "train_df['lang'] = train_df.Page.map(find_language)"
      ]
    },
    {
      "metadata": {
        "_uuid": "4e5baf613e8b435507a36d96a11c7e038de5e3f2",
        "_cell_guid": "93a78312-479e-43e6-b5e8-cfc562e04c55"
      },
      "cell_type": "markdown",
      "source": [
        "### Here we separate all the pages based on their language and average them up to find views per page per languag"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "2049fb6b9a5a82d6f22186e0f4a76ef84238ac31",
        "collapsed": true,
        "_cell_guid": "e78bfee2-98b3-4fd2-8d70-be26928f01e2"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "lang_sets = {}\n",
        "lang_sets['en'] = train_df[train_df.lang=='en'].iloc[:,0:-1]\n",
        "lang_sets['ja'] = train_df[train_df.lang=='ja'].iloc[:,0:-1]\n",
        "lang_sets['de'] = train_df[train_df.lang=='de'].iloc[:,0:-1]\n",
        "lang_sets['na'] = train_df[train_df.lang=='na'].iloc[:,0:-1]\n",
        "lang_sets['fr'] = train_df[train_df.lang=='fr'].iloc[:,0:-1]\n",
        "lang_sets['zh'] = train_df[train_df.lang=='zh'].iloc[:,0:-1]\n",
        "lang_sets['ru'] = train_df[train_df.lang=='ru'].iloc[:,0:-1]\n",
        "lang_sets['es'] = train_df[train_df.lang=='es'].iloc[:,0:-1]\n",
        "\n",
        "sums = {}\n",
        "for key in lang_sets:\n",
        "    sums[key] = lang_sets[key].iloc[:,1:].sum(axis=0) / lang_sets[key].shape[0]"
      ]
    },
    {
      "metadata": {
        "_uuid": "72f310df144bb407f9a2707593d279733f419351",
        "_cell_guid": "189f03e0-27c8-40b5-b85b-1755346ff276"
      },
      "cell_type": "markdown",
      "source": [
        "### Plots of average number of views for all different languages per day "
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "9872d9366c5a581a3d37500eeb81267850987de7",
        "_cell_guid": "dda148fb-ab53-4935-a962-b10441e04dee"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "days = [r for r in range(sums['en'].shape[0])]\n",
        "\n",
        "fig = plt.figure(1,figsize=[10,10])\n",
        "plt.ylabel('Views per Page')\n",
        "plt.xlabel('Day')\n",
        "plt.title('Pages in Different Languages')\n",
        "labels={'en':'English','ja':'Japanese','de':'German',\n",
        "        'na':'Media','fr':'French','zh':'Chinese',\n",
        "        'ru':'Russian','es':'Spanish'\n",
        "       }\n",
        "\n",
        "for key in sums:\n",
        "    plt.plot(days,sums[key],label = labels[key] )\n",
        "    \n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "metadata": {
        "_uuid": "80535b2fca75d2c25412b789af824dd0119f5486",
        "_cell_guid": "6b63e83a-55c3-4ee7-8c33-e87ba30ddc94"
      },
      "cell_type": "markdown",
      "source": [
        "### Now we can plot Autocorrelation and Partial Autocorrelation graphs for all these languages, to estimate the hyperparameters used in training the ARIMA model."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "4964b1bf73a89aa929d3b82b25511be686c030c2",
        "_cell_guid": "553f30cd-2d9f-4d41-ab0b-74fd49e755e2"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "from statsmodels.tsa.stattools import pacf\n",
        "from statsmodels.tsa.stattools import acf\n",
        "\n",
        "for key in sums:\n",
        "    fig = plt.figure(1,figsize=[10,5])\n",
        "    ax1 = fig.add_subplot(121)\n",
        "    ax2 = fig.add_subplot(122)\n",
        "    data = np.array(sums[key])\n",
        "    autocorr = acf(data)\n",
        "    pac = pacf(data)\n",
        "\n",
        "    x = [x for x in range(len(pac))]\n",
        "    ax1.plot(x[1:],autocorr[1:])\n",
        "\n",
        "    ax2.plot(x[1:],pac[1:])\n",
        "    ax1.set_xlabel('Lag')\n",
        "    ax1.set_ylabel('Autocorrelation')\n",
        "\n",
        "    ax2.set_xlabel('Lag')\n",
        "    ax2.set_ylabel('Partial Autocorrelation')\n",
        "    print(key)\n",
        "    plt.show()"
      ]
    },
    {
      "metadata": {
        "_uuid": "8a19984748d482717c1d2b3d4dc3195c58d3d5fd",
        "_cell_guid": "f9982671-4b9f-460a-a945-e11b0db92d13"
      },
      "cell_type": "markdown",
      "source": [
        "### Looking at all these graphs we conclude\n",
        " 1. We won't be needing any differencing for en, ru, fr, na (d=0). For the others we need to subtract them once from their predecessor (d=1).\n",
        " 2. For ja, de, zh, es there is a trend of peaks after 7 days. Thus a lag of 7 might be used and for the rest a lag of 4 should work okay."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "c19181bd2a7d4ad0c5808a89a0170b43cbb69f9f",
        "_cell_guid": "663b32a7-e0ec-44d3-92d2-5d9eb48a536a"
      },
      "cell_type": "markdown",
      "source": [
        "## Now we will be training ARIMA models for different languages"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "840ad13d6803387b2f2618871885eb017a43d7da",
        "_cell_guid": "3c212765-862c-4b11-9df1-e7f1ea55fc07"
      },
      "cell_type": "markdown",
      "source": [
        "We tune in the parameters discussed above and train our ARIMA models as follows:"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "ecc8041bcb019ecda1fd7c176664fe000fe4bb12",
        "_cell_guid": "40529eb1-79af-4eb5-8c49-d4527d1c7a6a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "params = {'en': [4,1,0], 'ja': [7,1,1], 'de': [7,1,1], 'na': [4,1,0], 'fr': [4,1,0], 'zh': [7,1,1], 'ru': [4,1,0], 'es': [7,1,1]}\n",
        "\n",
        "for key in sums:\n",
        "    data = np.array(sums[key])\n",
        "    result = None\n",
        "    arima = ARIMA(data,params[key])\n",
        "    result = arima.fit(disp=False)\n",
        "    #print(result.params)\n",
        "    pred = result.predict(2,599,typ='levels')\n",
        "    x = [i for i in range(600)]\n",
        "    i=0\n",
        "    \n",
        "    print(key)\n",
        "    plt.plot(x[2:len(data)],data[2:] ,label='Data')\n",
        "    plt.plot(x[2:],pred,label='ARIMA Model')\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Views')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "metadata": {
        "_uuid": "7b44c0e5e1b5035982e84c9008ea28fa8f1fa0c6",
        "collapsed": true,
        "_cell_guid": "51b2d657-bc96-42bb-b563-74e15019bf10"
      },
      "cell_type": "markdown",
      "source": [
        "### Now we will use the predictions of these models as one of the inputs to our ensemble model\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "556c8fd69cdf7aaaf0580a35da0c503143208c50",
        "collapsed": true,
        "_cell_guid": "57e8df14-1d6c-41a6-9d42-debb05041632"
      },
      "cell_type": "markdown",
      "source": [
        "### Now we will create another model to create an ensemble of the 2. Which will be a LSTM model.\n",
        "\n",
        "We will be using keras to train an LSTM model over the pages. "
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "57944f6ece133c079ace6b5056419cdf0a88a7f1",
        "_cell_guid": "5664c5e6-5905-49a3-8ff6-b53e15719cb4"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "train_df.head()"
      ]
    },
    {
      "metadata": {
        "_uuid": "23f6891dfd7ced83e359eddf7b91a7ee9b1bb9d3",
        "_cell_guid": "287adbc1-d528-4767-a6c7-8e7e069a3496"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### We will be training LSTM models for top pages of all the languages as a demo"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "d051db6dcb9531e07d489432ee433b79a6de162e",
        "_cell_guid": "899bf21b-f155-4cba-b5db-3a825b70f334"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "train_df = train_df.drop('Page',axis = 1)\n",
        "train_df.shape"
      ]
    },
    {
      "metadata": {
        "_uuid": "cc04998912631b6b518ccc051830d8c42cd9a9de",
        "_cell_guid": "ed76d361-8362-4aaa-abf8-1de17594e9a4"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "#Packages for pre processing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        " # Importing the Keras libraries and packages for LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM"
      ]
    },
    {
      "metadata": {
        "_uuid": "4a65bd5adf056ca55768340e014966d36a13cda8",
        "scrolled": true,
        "_cell_guid": "71cbc03a-06e7-4bcf-a369-03ccb8672068"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        " for key in sums:\n",
        "    row = [0]*sums[key].shape[0]\n",
        "    for i in range(sums[key].shape[0]):\n",
        "        row[i] = sums[key][i]\n",
        "\n",
        "\n",
        "    #Using Data From Random Row for Training and Testing\n",
        "\n",
        "    X = row[0:549]\n",
        "    y = row[1:550]\n",
        "\n",
        "    # Splitting the dataset into the Training set and Test set\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "    # Feature Scaling\n",
        "    sc = MinMaxScaler()\n",
        "    X_train = np.reshape(X_train,(-1,1))\n",
        "    y_train = np.reshape(y_train,(-1,1))\n",
        "    X_train = sc.fit_transform(X_train)\n",
        "    y_train = sc.fit_transform(y_train)\n",
        "\n",
        "\n",
        "    #Training LSTM\n",
        "\n",
        "    #Reshaping Array\n",
        "    X_train = np.reshape(X_train, (384,1,1))\n",
        "\n",
        "    # Initialising the RNN\n",
        "    regressor = Sequential()\n",
        "\n",
        "    # Adding the input layerand the LSTM layer\n",
        "    regressor.add(LSTM(units = 8, activation = 'relu', input_shape = (None, 1)))\n",
        "\n",
        "\n",
        "    # Adding the output layer\n",
        "    regressor.add(Dense(units = 1))\n",
        "\n",
        "    # Compiling the RNN\n",
        "    regressor.compile(optimizer = 'rmsprop', loss = 'mean_squared_error')\n",
        "\n",
        "    # Fitting the RNN to the Training set\n",
        "    regressor.fit(X_train, y_train, batch_size = 10, epochs = 100, verbose = 0)\n",
        "\n",
        "    # Getting the predicted Web View\n",
        "    inputs = X\n",
        "    inputs = np.reshape(inputs,(-1,1))\n",
        "    inputs = sc.transform(inputs)\n",
        "    inputs = np.reshape(inputs, (549,1,1))\n",
        "    y_pred = regressor.predict(inputs)\n",
        "    y_pred = sc.inverse_transform(y_pred)\n",
        "\n",
        "    print(key)\n",
        "    #Visualising Result\n",
        "    plt.figure\n",
        "    plt.plot(y, color = 'red', label = 'Real Web View')\n",
        "    plt.plot(y_pred, color = 'blue', label = 'Predicted Web View')\n",
        "    plt.title('Web View Forecasting')\n",
        "    plt.xlabel('Number of Days from Start')\n",
        "    plt.ylabel('Web View')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "metadata": {
        "_uuid": "b0672f3adb1af10e22561b4e6a113169d582308f",
        "_cell_guid": "a48475c0-6d98-42e8-8e92-d294b6307f0d"
      },
      "cell_type": "markdown",
      "source": [
        "### Now we will combine the 2 models and make an ensemble out of it in the next step"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "nbformat_minor": 1
}